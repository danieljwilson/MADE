## difference of 0.25 or less (try different values for this)
d <- d[abs(d$summed_val)<0.25,]
# Bias to weigh the face value more than justified?
# faceTotal/houseTotal = value
# total_0_face/total_1_house = fixation time
d <- v3
#d <- v3[v3$round_num>0,]
d$subject <- factor(d$subject)
sum(is.na(d$choice))
# subset trials with difficult choices
## opposite sign
## difference of 0.25 or less (try different values for this)
d <- d[abs(d$summed_val)<0.25,]
d <- v1
d$subject <- factor(d$subject)
# subset trials with difficult choices
## opposite sign
## difference of 0.25 or less (try different values for this)
d <- d[abs(d$summedVal)<0.25,]
View(d)
d <- d[sign(d$faceTotal) != sign(d$houseTotal),]
# remove learning trend
subs = unique(d$subject)
for (i in subs){
model <- lm(total_fix_house_1 ~ poly(trial,2), data = d[d$subject==i,])
d$resid_dwell_house[d$subject==i] = model$residuals
model <- lm(total_fix_face_0 ~ poly(trial,2), data = d[d$subject==i,])
d$resid_dwell_face[d$subject==i] = model$residuals
}
for (i in subs){
model <- lm(total_1_house ~ poly(trial,2), data = d[d$subject==i,])
d$resid_dwell_house[d$subject==i] = model$residuals
model <- lm(total_0_face ~ poly(trial,2), data = d[d$subject==i,])
d$resid_dwell_face[d$subject==i] = model$residuals
}
for (i in subs){
model <- lm(total_1_house ~ poly(Trial,2), data = d[d$subject==i,])
d$resid_dwell_house[d$subject==i] = model$residuals
model <- lm(total_0_face ~ poly(Trial,2), data = d[d$subject==i,])
d$resid_dwell_face[d$subject==i] = model$residuals
}
fit1 <- glmer(choice ~ faceTotal * resid_dwell_face + houseTotal * resid_dwell_house + (1|subject) + (faceTotal + resid_dwell_face|subject) + (houseTotal + resid_dwell_house | subject), family = binomial("logit"), data = d,
glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(fit1)
# Bias to weigh the face value more than justified?
# faceTotal/houseTotal = value
# total_0_face/total_1_house = fixation time
d <- v3
#d <- v3[v3$round_num>0,]
d$subject <- factor(d$subject)
# Remove nas
d <- d[!is.na(d$choice),]
sum(is.na(d$choice))
# subset trials with difficult choices
## opposite sign
## difference of 0.25 or less (try different values for this)
d <- d[abs(d$summed_val)<0.25,]
d <- d[sign(d$face_val_total) != sign(d$house_val_total),]
fit <- glmer(choice ~ face_val_total * total_fix_face_1 + house_val_total * total_fix_house_1 + (1|subject) + (face_val_total + total_fix_face_1|subject) + (house_val_total + total_fix_house_1 | subject), family = binomial("logit"), data = d,
glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
# Cendri's fit polynomial: Total value
fit <- glmer(choice ~ face_val_total * total_fix_face_0 + house_val_total * total_fix_house_1 + (1|subject) + (face_val_total + total_fix_face_1|subject) + (house_val_total + total_fix_house_1 | subject), family = binomial("logit"), data = d,
glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
# Cendri's fit polynomial: Total value
fit <- glmer(choice ~ face_val_total * total_fix_face_0 + house_val_total * total_fix_house_1 + (1|subject) + (face_val_total + total_fix_face_0|subject) + (house_val_total + total_fix_house_1 | subject), family = binomial("logit"), data = d,
glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
# Cendri's fit polynomial: Total value
fit <- glmer(choice ~ face_val_total * total_fix_face_0 + house_val_total * total_fix_house_1 + (1|subject) + (face_val_total + total_fix_face_0|subject) + (house_val_total + total_fix_house_1 | subject), family = binomial("logit"), data = d,
glmerControl(optimizer="bobyqa", optCtrl = list(maxfun = 100000)))
summary(fit)
# histogram bin size based on equal numbers of observations
d <- v3
d$choice[d$choice == -1] = 0 # -1 for Tavares needs to be 0 in order to calculate prob.
# How many bins?
numBins = 19 # same as Krajbich
# SUMMED VAL
d$valBin <- as.numeric(cut2(d$summedVal, g=numBins))
d$valBinAmt <- cut2(d$summedVal, g=numBins)
d$valBinCtr <- cut2(d$summedVal, g=numBins, levels.mean=TRUE)
vals = as.numeric(as.character(unique(d$valBinCtr)))
vals = sort(vals)
# FOR RT
subject_means_rt <- group_by(d, subject, valBinCtr) %>%
dplyr::summarize(rt = mean(rt, na.rm = T))
subject_means_rt
# FOR ACCURACY
subject_means_acc <- group_by(d, subject, valBinCtr) %>%
dplyr::summarize(correct = mean(correct, na.rm = T))
subject_means_acc
# FOR CHOICE
subject_means_choice <- group_by(d, subject, valBinCtr) %>%
dplyr::summarize(accept = mean(choice, na.rm = T))
subject_means_choice
# Create DF with all bins as columns
# FOR RT
subject_means_wide_rt <-
spread(subject_means_rt,
key = valBinCtr,
value = rt,
sep = "_")
# FOR ACCURACY
subject_means_wide_acc <-
spread(subject_means_acc,
key = valBinCtr,
value = correct,
sep = "_")
# FOR CHOICE
subject_means_wide_choice <-
spread(subject_means_choice,
key = valBinCtr,
value = accept,
sep = "_")
# DF with mean and SD for each bin
rt_x = sapply(subject_means_wide_rt, function(cl) list(means=mean(cl,na.rm=TRUE), sds=sd(cl,na.rm=TRUE)))
rt_x = t(rt_x)
acc_x = sapply(subject_means_wide_acc, function(cl) list(means=mean(cl,na.rm=TRUE), sds=sd(cl,na.rm=TRUE)))
acc_x = t(acc_x)
choice_x = sapply(subject_means_wide_choice, function(cl) list(means=mean(cl,na.rm=TRUE), sds=sd(cl,na.rm=TRUE)))
choice_x = t(choice_x)
# MEANs
rt_mean = numeric()
acc_mean = numeric()
choice_mean = numeric()
for(i in 2:20){
rt_mean = c(rt_mean, rt_x[i,1][[1]])
acc_mean = c(acc_mean, acc_x[i,1][[1]])
choice_mean = c(choice_mean, choice_x[i,1][[1]])
}
# SDs
rt_sd = numeric()
acc_sd = numeric()
choice_sd = numeric()
for(i in 2:20){
rt_sd = c(rt_sd, rt_x[i,2][[1]])
acc_sd = c(acc_sd, acc_x[i,2][[1]])
choice_sd = c(choice_sd, choice_x[i,2][[1]])
}
# Create DF
df = data.frame("val" = vals,
"rt_mean" = rt_mean, "rt_sd" = rt_sd,
"acc_mean" = acc_mean, "acc_sd" = acc_sd,
"choice_mean" = choice_mean, "choice_sd" = choice_sd)
# Add SEs
nVal = sqrt(length(unique(d$subject))) # calculate the denominator of the SE equation
df$rt_se <- df$rt_sd/nVal
df$acc_se <- df$acc_sd/nVal
df$choice_se <- df$choice_sd/nVal
# ACCURACY
ggplot(data = df,aes(x = val,y = acc_mean)) +
geom_point() +
#geom_line() +
geom_errorbar(aes(ymin = acc_mean-acc_se, ymax = acc_mean+acc_se)) +
labs(x = "Net Value", y = "p(Correct)") +
theme_minimal() +
ggtitle("B") +
theme(plot.title = element_text(size=22))
#  ggtitle("p(Correct) by Net Value ")
# CHOICE
ggplot(data=df, aes(x=val, y=choice_mean, group=factor(study), colour=factor(study))) +
geom_point() +
#geom_line() +
geom_errorbar(aes(ymin = choice_mean-choice_se, ymax = choice_mean+choice_se)) +
labs(x = "Net Value", y = "p(Accept)") +
scale_x_continuous(breaks = seq(-9,9,1)) +
theme_minimal() +
guides(colour=guide_legend("Study\nVersion")) +
theme(axis.title.x=element_text(size=17),
axis.title.y = element_text(size = 17))
#df <- v1_df
#df <- v2_df
df <- v3_df
# combined study df
df1 <- v1_df
df2 <- v3_df
df1$study = 1
df2$study = 2
df <- rbind(df1, df2)
# RT
ggplot(data = df,aes(x = val,y = rt_mean)) +
geom_point() +
#geom_line() +
geom_errorbar(aes(ymin = rt_mean-rt_se,ymax = rt_mean+rt_se)) +
labs(x = "Net Value", y = "Reaction Time (seconds)") +
theme_minimal() +
ggtitle("Mean Reaction Time by Net Value")
# CHOICE
ggplot(data=df, aes(x=val, y=choice_mean, group=factor(study), colour=factor(study))) +
geom_point() +
#geom_line() +
geom_errorbar(aes(ymin = choice_mean-choice_se, ymax = choice_mean+choice_se)) +
labs(x = "Net Value", y = "p(Accept)") +
scale_x_continuous(breaks = seq(-9,9,1)) +
theme_minimal() +
guides(colour=guide_legend("Study\nVersion")) +
theme(axis.title.x=element_text(size=17),
axis.title.y = element_text(size = 17))
#ggtitle("A") +
#theme(plot.title = element_text(size=22))
setwd("../../../../06_PRESENTATIONS/2018_SANS/Plots/")
ggsave("pAcc.pdf", width = 18, height = 12, units = "cm")
# ABS Val
d <- v3
d <- d[d$round_num>0,]
d$choice[d$choice == -1] = 0 # -1 for Tavares needs to be 0 in order to calculate prob.
d$absValBin <- as.numeric(cut2(d$absSummedVal, g=9))
d$absValBinAmt <- cut2(d$absSummedVal, g=9)
d$absValBinCtr <- cut2(d$absSummedVal, g=9, levels.mean=TRUE)
absVals = as.numeric(as.character(unique(d$absValBinCtr)))
absVals = sort(absVals)
# Abs Val RT
abs_subject_means_rt <- group_by(d, subject, absValBinCtr) %>%
dplyr::summarize(rt = mean(rt, na.rm = T))
abs_subject_means_rt
# FOR Fixations
abs_subject_means_fix <- group_by(d, subject, absValBinCtr) %>%
dplyr::summarize(fixations = mean(swapAmount, na.rm = T))
abs_subject_means_fix
# Create DF with all bins as columns
# FOR RT
abs_subject_means_wide_rt <-
spread(abs_subject_means_rt,
key = absValBinCtr,
value = rt,
sep = "_")
# FOR ACCURACY
abs_subject_means_wide_fix <-
spread(abs_subject_means_fix,
key = absValBinCtr,
value = fixations,
sep = "_")
# DF with mean and SD for each bin
rt_x = sapply(abs_subject_means_wide_rt, function(cl) list(means=mean(cl,na.rm=TRUE), sds=sd(cl,na.rm=TRUE)))
rt_x = t(rt_x)
fix_x = sapply(abs_subject_means_wide_fix, function(cl) list(means=mean(cl,na.rm=TRUE), sds=sd(cl,na.rm=TRUE)))
fix_x = t(fix_x)
rt_x
rt_mean = numeric()
fix_mean = numeric()
for(i in 2:10){   # for some reason this needs to be 2:10 for v1 and v3 but 2:9 for v2
rt_mean = c(rt_mean, rt_x[i,1][[1]])
fix_mean = c(fix_mean, fix_x[i,1][[1]])
}
rt_sd = numeric()
fix_sd = numeric()
for(i in 2:10){
rt_sd = c(rt_sd, rt_x[i,2][[1]])
fix_sd = c(fix_sd, fix_x[i,2][[1]])
}
df = data.frame("abs_val" = absVals, "rt_mean" = rt_mean, "rt_sd" = rt_sd, "fix_mean" = fix_mean, "fix_sd" = fix_sd)
nVal = sqrt(length(unique(d$subject))) # calculate the denominator of the SE equation
df$rt_se <- df$rt_sd/nVal
df$fix_se <- df$fix_sd/nVal
df
#df <- v1_df_abs
#df <- v2_df_abs
#df <- v3_df_abs
# combined study df
df1 <- v1_df_abs
df2 <- v3_df_abs
df1$study = 1
df2$study = 2
df <- rbind(df1, df2)
ggplot(data = df,aes(x = abs_val, y = rt_mean, group=factor(study), colour=factor(study))) +
geom_point() +
#geom_line() +
geom_errorbar(aes(ymin = rt_mean-rt_se,ymax = rt_mean+rt_se)) +
labs(x = "Absolute Net Value ($)", y = "Reaction Time (s)") +
scale_x_continuous(breaks = seq(0,max(df$abs_val),0.5)) +
scale_y_continuous(breaks = seq(2.2,max(df$rt_mean)+0.2,0.2)) +
theme_minimal()+
guides(colour=guide_legend("Study\nVersion")) +
theme(axis.title.x=element_text(size=18),
axis.title.y = element_text(size = 18))
#ggtitle("Mean Reaction Time by Absolute Net Value")
setwd("../../../../06_PRESENTATIONS/2018_SANS/Plots/")
ggsave("RT_v3.pdf", width = 18, height = 12, units = "cm")
ggplot(data = df,aes(x = abs_val,y = fix_mean, group=factor(study), colour=factor(study))) +
geom_point() +
#geom_line() +
geom_errorbar(aes(ymin = fix_mean-fix_se, ymax = fix_mean+fix_se)) +
labs(x = "Absolute Net Value ($)", y = "Fixation Count") +
scale_x_continuous(breaks = seq(0,max(df$abs_val),0.5)) +
scale_y_continuous(breaks = seq(2.4,max(df$fix_mean)+0.1,0.1)) +
theme_minimal()+
guides(colour=guide_legend("Study\nVersion")) +
theme(axis.title.x=element_text(size=17),
axis.title.y = element_text(size = 17))
ggtitle("Fixations by Absolute Net Value ")
setwd("../../../../06_PRESENTATIONS/2018_SANS/Plots/")
ggsave("Fix_Num_v3.pdf", width = 18, height = 12, units = "cm")
# Create regressors for each multipier
d <- v3
d$face_mult_01 = d$face_mult_05 = d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = d$face_mult_10 = NA
d$house_mult_01 = d$house_mult_05 = d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = d$house_mult_10 = NA
d$face_mult_01 = d$face_mult_05 = d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = d$face_mult_10 = 0
d$house_mult_01 = d$house_mult_05 = d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = d$house_mult_10 = 0
d$face_mult_01 = d$face_mult_05 = d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = d$face_mult_10 = 0
d$house_mult_01 = d$house_mult_05 = d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = d$house_mult_10 = 0
d$face_mult_01 <- d$face_val_base[d$face_mult == 0.1,]
d$face_mult_01 <- d$face_val_base[d$face_mult == 0.1]
d$face_mult_01 <- d$face_val_base[d$face_mult == 0.1,]
View(d)
d$face_val_base[d$face_mult == 0.1,]
d$face_val_base[d$face_mult == 0.1]
d$face_mult_01[d$face_mult == 0.1] <- d$face_val_base[d$face_mult == 0.1]
d$face_mult_01 = d$face_mult_05 = d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = d$face_mult_10 = 0
d$house_mult_01 = d$house_mult_05 = d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = d$house_mult_10 = 0
d$face_mult_01[d$face_mult == 0.1] <- d$face_val_base[d$face_mult == 0.1]
d$face_mult_01[d$face_mult == 0.5] <- d$face_val_base[d$face_mult == 0.5]
d$face_mult_01[d$face_mult == 1] <- d$face_val_base[d$face_mult == 1]
d$face_mult_01[d$face_mult == 2] <- d$face_val_base[d$face_mult == 2]
d$face_mult_01[d$face_mult == 3] <- d$face_val_base[d$face_mult == 3]
d$face_mult_01[d$face_mult == 10] <- d$face_val_base[d$face_mult == 10]
d$house_mult_01[d$house_mult == 0.1] <- d$house_val_base[d$house_mult == 0.1]
d$house_mult_01[d$house_mult == 0.5] <- d$house_val_base[d$house_mult == 0.5]
d$house_mult_01[d$house_mult == 1] <- d$house_val_base[d$house_mult == 1]
d$house_mult_01[d$house_mult == 2] <- d$house_val_base[d$house_mult == 2]
d$house_mult_01[d$house_mult == 3] <- d$house_val_base[d$house_mult == 3]
d$house_mult_01[d$house_mult == 10] <- d$house_val_base[d$house_mult == 10]
d$face_mult_01 = d$face_mult_05 = d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = d$face_mult_10 = 0
d$house_mult_01 = d$house_mult_05 = d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = d$house_mult_10 = 0
d$face_mult_01[d$face_mult == 0.1] <- d$face_val_base[d$face_mult == 0.1]
d$face_mult_05[d$face_mult == 0.5] <- d$face_val_base[d$face_mult == 0.5]
d$face_mult_1[d$face_mult == 1] <- d$face_val_base[d$face_mult == 1]
d$face_mult_2[d$face_mult == 2] <- d$face_val_base[d$face_mult == 2]
d$face_mult_3[d$face_mult == 3] <- d$face_val_base[d$face_mult == 3]
d$face_mult_10[d$face_mult == 10] <- d$face_val_base[d$face_mult == 10]
d$house_mult_01[d$house_mult == 0.1] <- d$house_val_base[d$house_mult == 0.1]
d$house_mult_05[d$house_mult == 0.5] <- d$house_val_base[d$house_mult == 0.5]
d$house_mult_1[d$house_mult == 1] <- d$house_val_base[d$house_mult == 1]
d$house_mult_2[d$house_mult == 2] <- d$house_val_base[d$house_mult == 2]
d$house_mult_3[d$house_mult == 3] <- d$house_val_base[d$house_mult == 3]
d$house_mult_10[d$house_mult == 10] <- d$house_val_base[d$house_mult == 10]
glm_fit <- glmer(choice ~ d$face_mult_01 + d$face_mult_05 + d$face_mult_1 + d$face_mult_2 + d$face_mult_3 + d$face_mult_10
+ d$house_mult_01 + d$house_mult_05 + d$house_mult_1 + d$house_mult_2 + d$house_mult_3 + d$house_mult_10,
+ (1|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
View(d)
colnames(d)
glm_fit <- glmer(choice ~ d$face_mult_01 + d$face_mult_05 + d$face_mult_1 + d$face_mult_2 + d$face_mult_3 + d$face_mult_10
+ d$house_mult_01 + d$house_mult_05 + d$house_mult_1 + d$house_mult_2 + d$house_mult_3 + d$house_mult_10,
+ (1|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
d$subject
glm_fit <- glmer(choice ~ face_mult_01 + face_mult_05 + face_mult_1 + face_mult_2 + face_mult_3 + face_mult_10
+ house_mult_01 + house_mult_05 + house_mult_1 + house_mult_2 + house_mult_3 + house_mult_10,
+ (1|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
glm_fit <- glmer(choice ~ face_mult_01 + face_mult_05 + face_mult_1 + face_mult_2 + face_mult_3 + face_mult_10
+ house_mult_01 + house_mult_05 + house_mult_1 + house_mult_2 + house_mult_3 + house_mult_10
+ (1|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
summary(glm_fit)
glm_fit <- glmer(choice ~ face_mult_01 + face_mult_05 + face_mult_1 + face_mult_2 + face_mult_3 + face_mult_10
+ house_mult_01 + house_mult_05 + house_mult_1 + house_mult_2 + house_mult_3 + house_mult_10
+ (1|subject) + (face_mult_01 + face_mult_05 + face_mult_1 + face_mult_2 + face_mult_3 + face_mult_10
+ house_mult_01 + house_mult_05 + house_mult_1 + house_mult_2 + house_mult_3 + house_mult_10|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
5 + 5
# Create regressors for each multipier
d <- v3
d$face_mult_01 = d$face_mult_05 = d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = d$face_mult_10 = 0
d$house_mult_01 = d$house_mult_05 = d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = d$house_mult_10 = 0
# Create regressors for each multipier
d <- v1
d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = 0
d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = 0
d$face_mult_1[d$face_mult == 1] <- d$face_val_base[d$face_mult == 1]
d$face_mult_2[d$face_mult == 2] <- d$face_val_base[d$face_mult == 2]
d$face_mult_3[d$face_mult == 3] <- d$face_val_base[d$face_mult == 3]
d$house_mult_1[d$house_mult == 1] <- d$house_val_base[d$house_mult == 1]
d$house_mult_2[d$house_mult == 2] <- d$house_val_base[d$house_mult == 2]
d$house_mult_3[d$house_mult == 3] <- d$house_val_base[d$house_mult == 3]
glm_fit <- glmer(choice ~ face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3
+ (1|subject) + (face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
# Create regressors for each multipier
d <- v1
d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = 0
d$face_mult_1[d$mult2Face == 1] <- d$faceVal[d$mult2Face == 1]
d$face_mult_2[d$mult2Face == 2] <- d$faceVal[d$mult2Face == 2]
# Create regressors for each multipier
d <- v1
d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = 0
d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = 0
d$face_mult_1[d$mult2Face == 1] <- d$faceVal[d$mult2Face == 1]
d$face_mult_2[d$mult2Face == 2] <- d$faceVal[d$mult2Face == 2]
d$face_mult_3[d$mult2Face == 3] <- d$faceVal[d$mult2Face == 3]
d$house_mult_1[d$mult1House == 1] <- d$houseVal[d$mult1House == 1]
d$house_mult_2[d$mult1House == 2] <- d$houseVal[d$mult1House == 2]
d$house_mult_3[d$mult1House == 3] <- d$houseVal[d$mult1House == 3]
glm_fit <- glmer(choice ~ face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3
+ (1|subject) + (face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
glm_fit <- glmer(choice ~ face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3
+ (1|subject) + (face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
summary(glm_fit_v1)
summary(glm_fit)
glm_fit$coeffients
sjp.glmer(glm_fit,
type = "ri.slope",
facet.grid = FALSE)
sjp.glmer(fit,
facet.grid = FALSE,
sort.est = "sort.all",
y.offset = .4)
# Create regressors for each multipier
d <- v1
d$face_mult_1 = d$face_mult_2 = d$face_mult_3 = 0
d$house_mult_1 = d$house_mult_2 = d$house_mult_3 = 0
d$face_mult_1[d$mult2Face == 1] <- d$faceVal[d$mult2Face == 1]
d$face_mult_2[d$mult2Face == 2] <- d$faceVal[d$mult2Face == 2]
d$face_mult_3[d$mult2Face == 3] <- d$faceVal[d$mult2Face == 3]
d$house_mult_1[d$mult1House == 1] <- d$houseVal[d$mult1House == 1]
d$house_mult_2[d$mult1House == 2] <- d$houseVal[d$mult1House == 2]
d$house_mult_3[d$mult1House == 3] <- d$houseVal[d$mult1House == 3]
glm_fit <- glmer(choice ~ face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3
+ (1|subject) + (face_mult_1 + face_mult_2 + face_mult_3
+ house_mult_1 + house_mult_2 + house_mult_3|subject),
data = d,
family = binomial("logit"),
glmerControl(optimizer="bobyqa",
optCtrl = list(maxfun = 100000))
)
summary(glm_fit)
sjp.glmer(glm_fit,
facet.grid = FALSE,
sort.est = "sort.all",
y.offset = .4)
sjp.glmer(glm_fit, y.offset = .4)
sjp.glmer(glm_fit, type = "fe")
sjp.glmer(glm_fit, type = "re.qq")
sjp.glmer(glm_fit,
type = "ri.slope",
facet.grid = FALSE)
sjp.glmer(glm_fit,
facet.grid = FALSE,
sort.est = "sort.all",
y.offset = .4)
lattice::dotplot(ranef(glm_fit))
devtools::install_github("sjPlot/devel")
install.packages(c('devtools','curl'))
install.packages(c("devtools", "curl"))
devtools::install_github("sjPlot/devel")
knitr::opts_chunk$set(echo = FALSE)
sjp.lmer(glm_fit, type = "re", show.ci = F, sort.est = "(Intercept)")
library(magrittr)
library(ggplot2)
library(dplyr)
library(ggplot2)
library(lme4)
library(tidyr)
library(merTools)
library(lsr)
library(reshape2)
library(dtplyr)
library(data.table)
library(Hmisc) # cut2 function
library(lme4)
library(nlme)
library(sjPlot)
install.packages("sjPlot")
knitr::opts_chunk$set(echo = FALSE)
library(sjPlot)
install.packages("pbkrtest")
library(sjmisc)
library(sjPlot)
remove.packages(sjPlot)
install.packages("sjPlot")
install.packages("TMB",type="source")
install.packages("TMB", type = "source")
library(sjPlot)
